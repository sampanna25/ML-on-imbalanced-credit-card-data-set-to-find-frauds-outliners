{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1nii0e7Zx4E"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "import matplotlib.patches as mpatches\n",
        "import time\n",
        "\n",
        "# Classifier Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import collections\n",
        "\n",
        "\n",
        "# Other Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "df = pd.read_csv('../input/creditcard.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "hUe12u0jZyOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum().max()\n",
        "#no null values"
      ],
      "metadata": {
        "id": "GrGwdVjCZ46P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "UTw15SlLZ-YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
        "print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')"
      ],
      "metadata": {
        "id": "z9lhE42raAJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [\"#0101DF\", \"#DF0101\"]\n",
        "\n",
        "sns.countplot('Class', data=df, palette=colors)\n",
        "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14"
      ],
      "metadata": {
        "id": "Wwg7U9AZaCrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
        "\n",
        "amount_val = df['Amount'].values\n",
        "time_val = df['Time'].values\n",
        "\n",
        "sns.distplot(amount_val, ax=ax[0], color='r')\n",
        "ax[0].set_title('Distribution of Transaction Amount', fontsize=14)\n",
        "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
        "\n",
        "sns.distplot(time_val, ax=ax[1], color='b')\n",
        "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
        "ax[1].set_xlim([min(time_val), max(time_val)])\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TGxlIwp-aI4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "\n",
        "# RobustScaler is less prone to outliers.\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "rob_scaler = RobustScaler()\n",
        "\n",
        "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
        "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
        "\n",
        "df.drop(['Time','Amount'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "qvx_Y6XgaK7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_amount = df['scaled_amount']\n",
        "scaled_time = df['scaled_time']\n",
        "\n",
        "df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
        "df.insert(0, 'scaled_amount', scaled_amount)\n",
        "df.insert(1, 'scaled_time', scaled_time)\n",
        "\n",
        "# Amount and Time are Scaled!\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "I63MFSSsaNC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
        "print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n",
        "\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
        "\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
        "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
        "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
        "# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n",
        "# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the Distribution of the labels\n",
        "\n",
        "\n",
        "# Turn into an array\n",
        "original_Xtrain = original_Xtrain.values\n",
        "original_Xtest = original_Xtest.values\n",
        "original_ytrain = original_ytrain.values\n",
        "original_ytest = original_ytest.values\n",
        "\n",
        "# See if both the train and test label distribution are similarly distributed\n",
        "train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n",
        "test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n",
        "print('-' * 100)\n",
        "\n",
        "print('Label Distributions: \\n')\n",
        "print(train_counts_label/ len(original_ytrain))\n",
        "print(test_counts_label/ len(original_ytest))\n"
      ],
      "metadata": {
        "id": "pYuwINmoaOu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n",
        "\n",
        "# Lets shuffle the data before creating the subsamples\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# amount of fraud classes 492 rows.\n",
        "fraud_df = df.loc[df['Class'] == 1]\n",
        "non_fraud_df = df.loc[df['Class'] == 0][:492]\n",
        "\n",
        "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
        "\n",
        "# Shuffle dataframe rows\n",
        "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
        "\n",
        "new_df.head()"
      ],
      "metadata": {
        "id": "MsexaD55aWrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Distribution of the Classes in the subsample dataset')\n",
        "print(new_df['Class'].value_counts()/len(new_df))\n",
        "\n",
        "\n",
        "\n",
        "sns.countplot('Class', data=new_df, palette=colors)\n",
        "plt.title('Equally Distributed Classes', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eKIcgAGraaQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Make sure we use the subsample in our correlation\n",
        "\n",
        "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
        "\n",
        "# Entire DataFrame\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\n",
        "ax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n",
        "\n",
        "\n",
        "sub_sample_corr = new_df.corr()\n",
        "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\n",
        "ax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PYrRzSq6aay_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
        "\n",
        "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
        "sns.boxplot(x=\"Class\", y=\"V17\", data=new_df, palette=colors, ax=axes[0])\n",
        "axes[0].set_title('V17 vs Class Negative Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1])\n",
        "axes[1].set_title('V14 vs Class Negative Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[2])\n",
        "axes[2].set_title('V12 vs Class Negative Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=colors, ax=axes[3])\n",
        "axes[3].set_title('V10 vs Class Negative Correlation')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2vPaFl3racnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n",
        "\n",
        "v14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values\n",
        "sns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\n",
        "ax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
        "\n",
        "v12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values\n",
        "sns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\n",
        "ax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
        "\n",
        "\n",
        "v10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values\n",
        "sns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\n",
        "ax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-NkVM1gbafHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\n",
        "v14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\n",
        "q25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\n",
        "print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n",
        "v14_iqr = q75 - q25\n",
        "print('iqr: {}'.format(v14_iqr))\n",
        "\n",
        "v14_cut_off = v14_iqr * 1.5\n",
        "v14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\n",
        "print('Cut Off: {}'.format(v14_cut_off))\n",
        "print('V14 Lower: {}'.format(v14_lower))\n",
        "print('V14 Upper: {}'.format(v14_upper))\n",
        "\n",
        "outliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\n",
        "print('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\n",
        "print('V10 outliers:{}'.format(outliers))\n",
        "\n",
        "new_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\n",
        "print('----' * 44)\n",
        "\n",
        "# -----> V12 removing outliers from fraud transactions\n",
        "v12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\n",
        "q25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\n",
        "v12_iqr = q75 - q25\n",
        "\n",
        "v12_cut_off = v12_iqr * 1.5\n",
        "v12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\n",
        "print('V12 Lower: {}'.format(v12_lower))\n",
        "print('V12 Upper: {}'.format(v12_upper))\n",
        "outliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\n",
        "print('V12 outliers: {}'.format(outliers))\n",
        "print('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\n",
        "new_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\n",
        "print('Number of Instances after outliers removal: {}'.format(len(new_df)))\n",
        "print('----' * 44)\n",
        "\n",
        "\n",
        "# Removing outliers V10 Feature\n",
        "v10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\n",
        "q25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\n",
        "v10_iqr = q75 - q25\n",
        "\n",
        "v10_cut_off = v10_iqr * 1.5\n",
        "v10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\n",
        "print('V10 Lower: {}'.format(v10_lower))\n",
        "print('V10 Upper: {}'.format(v10_upper))\n",
        "outliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\n",
        "print('V10 outliers: {}'.format(outliers))\n",
        "print('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\n",
        "new_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\n",
        "print('Number of Instances after outliers removal: {}'.format(len(new_df)))"
      ],
      "metadata": {
        "id": "OrEjGTjeaiA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n",
        "\n",
        "colors = ['#B3F9C5', '#f9c5b3']\n",
        "# Boxplots with outliers removed\n",
        "# Feature V14\n",
        "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df,ax=ax1, palette=colors)\n",
        "ax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\n",
        "ax1.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n",
        "            arrowprops=dict(facecolor='black'),\n",
        "            fontsize=14)\n",
        "\n",
        "# Feature 12\n",
        "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax2, palette=colors)\n",
        "ax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\n",
        "ax2.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.3), xytext=(0, -12),\n",
        "            arrowprops=dict(facecolor='black'),\n",
        "            fontsize=14)\n",
        "\n",
        "# Feature V10\n",
        "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax3, palette=colors)\n",
        "ax3.set_title(\"V10 Feature \\n Reduction of outliers\", fontsize=14)\n",
        "ax3.annotate('Fewer extreme \\n outliers', xy=(0.95, -16.5), xytext=(0, -12),\n",
        "            arrowprops=dict(facecolor='black'),\n",
        "            fontsize=14)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VtRIq-UAasHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# New_df is from the random undersample data (fewer instances)\n",
        "X = new_df.drop('Class', axis=1)\n",
        "y = new_df['Class']\n",
        "\n",
        "\n",
        "# T-SNE Implementation\n",
        "t0 = time.time()\n",
        "X_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n",
        "t1 = time.time()\n",
        "print(\"T-SNE took {:.2} s\".format(t1 - t0))\n",
        "\n",
        "# PCA Implementation\n",
        "t0 = time.time()\n",
        "X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\n",
        "t1 = time.time()\n",
        "print(\"PCA took {:.2} s\".format(t1 - t0))\n",
        "\n",
        "# TruncatedSVD\n",
        "t0 = time.time()\n",
        "X_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\n",
        "t1 = time.time()\n",
        "print(\"Truncated SVD took {:.2} s\".format(t1 - t0))"
      ],
      "metadata": {
        "id": "m2v0nmxYavY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Undersampling before cross validating (prone to overfit)\n",
        "X = new_df.drop('Class', axis=1)\n",
        "y = new_df['Class']"
      ],
      "metadata": {
        "id": "ClzpumQRazcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our data is already scaled we should split our training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# This is explicitly used for undersampling.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "CfSYOikta1IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the values into an array for feeding the classification algorithms.\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values"
      ],
      "metadata": {
        "id": "tlRCXhgIa2j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's implement simple classifiers\n",
        "\n",
        "classifiers = {\n",
        "    \"LogisiticRegression\": LogisticRegression(),\n",
        "    \"KNearest\": KNeighborsClassifier(),\n",
        "    \"Support Vector Classifier\": SVC(),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
        "}"
      ],
      "metadata": {
        "id": "7-BKkoCZa4qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Wow our scores are getting even high scores even when applying cross validation.\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "for key, classifier in classifiers.items():\n",
        "    classifier.fit(X_train, y_train)\n",
        "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
        "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
      ],
      "metadata": {
        "id": "NWLInJ9ga5IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GridSearchCV to find the best parameters.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "\n",
        "\n",
        "\n",
        "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
        "grid_log_reg.fit(X_train, y_train)\n",
        "# We automatically get the logistic regression with the best parameters.\n",
        "log_reg = grid_log_reg.best_estimator_\n",
        "\n",
        "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
        "\n",
        "grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\n",
        "grid_knears.fit(X_train, y_train)\n",
        "# KNears best estimator\n",
        "knears_neighbors = grid_knears.best_estimator_\n",
        "\n",
        "# Support Vector Classifier\n",
        "svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
        "grid_svc = GridSearchCV(SVC(), svc_params)\n",
        "grid_svc.fit(X_train, y_train)\n",
        "\n",
        "# SVC best estimator\n",
        "svc = grid_svc.best_estimator_\n",
        "\n",
        "# DecisionTree Classifier\n",
        "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)),\n",
        "              \"min_samples_leaf\": list(range(5,7,1))}\n",
        "grid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\n",
        "grid_tree.fit(X_train, y_train)\n",
        "\n",
        "# tree best estimator\n",
        "tree_clf = grid_tree.best_estimator_"
      ],
      "metadata": {
        "id": "ssw0S-Hka67M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overfitting Case\n",
        "\n",
        "log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\n",
        "print('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n",
        "\n",
        "\n",
        "knears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\n",
        "print('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n",
        "\n",
        "svc_score = cross_val_score(svc, X_train, y_train, cv=5)\n",
        "print('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n",
        "\n",
        "tree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\n",
        "print('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')"
      ],
      "metadata": {
        "id": "n68F9p32a9X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# We will undersample during cross validating\n",
        "undersample_X = df.drop('Class', axis=1)\n",
        "undersample_y = df['Class']\n",
        "\n",
        "for train_index, test_index in sss.split(undersample_X, undersample_y):\n",
        "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
        "    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n",
        "    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n",
        "\n",
        "undersample_Xtrain = undersample_Xtrain.values\n",
        "undersample_Xtest = undersample_Xtest.values\n",
        "undersample_ytrain = undersample_ytrain.values\n",
        "undersample_ytest = undersample_ytest.values\n",
        "\n",
        "undersample_accuracy = []\n",
        "undersample_precision = []\n",
        "undersample_recall = []\n",
        "undersample_f1 = []\n",
        "undersample_auc = []\n",
        "\n",
        "# Implementing NearMiss Technique\n",
        "# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\n",
        "X_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\n",
        "print('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n",
        "# Cross Validating the right way\n",
        "\n",
        "for train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n",
        "    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n",
        "    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n",
        "    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n",
        "\n",
        "    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
        "    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n",
        "    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n",
        "    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n",
        "    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))"
      ],
      "metadata": {
        "id": "nMCa870ha_Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Plot LogisticRegression Learning Curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "def plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    # First Estimator\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"#ff9124\")\n",
        "    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
        "    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
        "             label=\"Training score\")\n",
        "    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
        "             label=\"Cross-validation score\")\n",
        "    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n",
        "    ax1.set_xlabel('Training size (m)')\n",
        "    ax1.set_ylabel('Score')\n",
        "    ax1.grid(True)\n",
        "    ax1.legend(loc=\"best\")\n",
        "\n",
        "    # Second Estimator\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"#ff9124\")\n",
        "    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
        "    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
        "             label=\"Training score\")\n",
        "    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
        "             label=\"Cross-validation score\")\n",
        "    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n",
        "    ax2.set_xlabel('Training size (m)')\n",
        "    ax2.set_ylabel('Score')\n",
        "    ax2.grid(True)\n",
        "    ax2.legend(loc=\"best\")\n",
        "\n",
        "    # Third Estimator\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"#ff9124\")\n",
        "    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
        "    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
        "             label=\"Training score\")\n",
        "    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
        "             label=\"Cross-validation score\")\n",
        "    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n",
        "    ax3.set_xlabel('Training size (m)')\n",
        "    ax3.set_ylabel('Score')\n",
        "    ax3.grid(True)\n",
        "    ax3.legend(loc=\"best\")\n",
        "\n",
        "    # Fourth Estimator\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"#ff9124\")\n",
        "    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
        "    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
        "             label=\"Training score\")\n",
        "    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
        "             label=\"Cross-validation score\")\n",
        "    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n",
        "    ax4.set_xlabel('Training size (m)')\n",
        "    ax4.set_ylabel('Score')\n",
        "    ax4.grid(True)\n",
        "    ax4.legend(loc=\"best\")\n",
        "    return plt"
      ],
      "metadata": {
        "id": "XJMdGioYbCRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# Create a DataFrame with all the scores and the classifiers names.\n",
        "\n",
        "log_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n",
        "                             method=\"decision_function\")\n",
        "\n",
        "knears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n",
        "\n",
        "svc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n",
        "                             method=\"decision_function\")\n",
        "\n",
        "tree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)"
      ],
      "metadata": {
        "id": "jyKwPmTJbIRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\n",
        "print('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\n",
        "print('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\n",
        "print('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))"
      ],
      "metadata": {
        "id": "8dwlcy43bNt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\n",
        "knear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\n",
        "svc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\n",
        "tree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n",
        "\n",
        "\n",
        "def graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n",
        "    plt.figure(figsize=(16,8))\n",
        "    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n",
        "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n",
        "    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n",
        "    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n",
        "    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.axis([-0.01, 1, 0, 1])\n",
        "    plt.xlabel('False Positive Rate', fontsize=16)\n",
        "    plt.ylabel('True Positive Rate', fontsize=16)\n",
        "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
        "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
        "                )\n",
        "    plt.legend()\n",
        "\n",
        "graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BeJiwV9qbSy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_roc_curve(log_fpr, log_tpr):\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.title('Logistic Regression ROC Curve', fontsize=16)\n",
        "    plt.plot(log_fpr, log_tpr, 'b-', linewidth=2)\n",
        "    plt.plot([0, 1], [0, 1], 'r--')\n",
        "    plt.xlabel('False Positive Rate', fontsize=16)\n",
        "    plt.ylabel('True Positive Rate', fontsize=16)\n",
        "    plt.axis([-0.01,1,0,1])\n",
        "\n",
        "\n",
        "logistic_roc_curve(log_fpr, log_tpr)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxsiTJMjbTQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)"
      ],
      "metadata": {
        "id": "qZ39A2fWbVzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "undersample_y_score = log_reg.decision_function(original_Xtest)"
      ],
      "metadata": {
        "id": "FNIGUMJubXgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "undersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n",
        "\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      undersample_average_precision))"
      ],
      "metadata": {
        "id": "SoSyyBMsbajT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(original_ytest, undersample_y_score)\n",
        "\n",
        "plt.step(recall, precision, color='#004a93', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
        "                 color='#48a6ff')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
        "          undersample_average_precision), fontsize=16)"
      ],
      "metadata": {
        "id": "7qqelFy6bdIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "\n",
        "print('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\n",
        "print('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n",
        "\n",
        "# List to append the score and then find the average\n",
        "accuracy_lst = []\n",
        "precision_lst = []\n",
        "recall_lst = []\n",
        "f1_lst = []\n",
        "auc_lst = []\n",
        "\n",
        "# Classifier with optimal parameters\n",
        "# log_reg_sm = grid_log_reg.best_estimator_\n",
        "log_reg_sm = LogisticRegression()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n",
        "\n",
        "\n",
        "# Implementing SMOTE Technique\n",
        "# Cross Validating the right way\n",
        "# Parameters\n",
        "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "for train, test in sss.split(original_Xtrain, original_ytrain):\n",
        "    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n",
        "    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n",
        "    best_est = rand_log_reg.best_estimator_\n",
        "    prediction = best_est.predict(original_Xtrain[test])\n",
        "\n",
        "    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
        "    precision_lst.append(precision_score(original_ytrain[test], prediction))\n",
        "    recall_lst.append(recall_score(original_ytrain[test], prediction))\n",
        "    f1_lst.append(f1_score(original_ytrain[test], prediction))\n",
        "    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n",
        "\n",
        "print('---' * 45)\n",
        "print('')\n",
        "print(\"accuracy: {}\".format(np.mean(accuracy_lst)))\n",
        "print(\"precision: {}\".format(np.mean(precision_lst)))\n",
        "print(\"recall: {}\".format(np.mean(recall_lst)))\n",
        "print(\"f1: {}\".format(np.mean(f1_lst)))\n",
        "print('---' * 45)"
      ],
      "metadata": {
        "id": "gGHQacVabfHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['No Fraud', 'Fraud']\n",
        "smote_prediction = best_est.predict(original_Xtest)\n",
        "print(classification_report(original_ytest, smote_prediction, target_names=labels))"
      ],
      "metadata": {
        "id": "IxdIojK8br1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_score = best_est.decision_function(original_Xtest)"
      ],
      "metadata": {
        "id": "5yDp-X1xbtud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_precision = average_precision_score(original_ytest, y_score)\n",
        "\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))"
      ],
      "metadata": {
        "id": "lqKgscOebv1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(original_ytest, y_score)\n",
        "\n",
        "plt.step(recall, precision, color='r', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
        "                 color='#F59B00')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
        "          average_precision), fontsize=16)"
      ],
      "metadata": {
        "id": "zV1oAleCbxyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SMOTE Technique (OverSampling) After splitting and Cross Validating\n",
        "sm = SMOTE(ratio='minority', random_state=42)\n",
        "# Xsm_train, ysm_train = sm.fit_sample(X_train, y_train)\n",
        "\n",
        "\n",
        "# This will be the data were we are going to\n",
        "Xsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)"
      ],
      "metadata": {
        "id": "5EUtPu5gbzzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We Improve the score by 2% points approximately\n",
        "# Implement GridSearchCV and the other models.\n",
        "\n",
        "# Logistic Regression\n",
        "t0 = time.time()\n",
        "log_reg_sm = grid_log_reg.best_estimator_\n",
        "log_reg_sm.fit(Xsm_train, ysm_train)\n",
        "t1 = time.time()\n",
        "print(\"Fitting oversample data took :{} sec\".format(t1 - t0))"
      ],
      "metadata": {
        "id": "dxHBKgxPb153"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Logistic Regression fitted using SMOTE technique\n",
        "y_pred_log_reg = log_reg_sm.predict(X_test)\n",
        "\n",
        "# Other models fitted with UnderSampling\n",
        "y_pred_knear = knears_neighbors.predict(X_test)\n",
        "y_pred_svc = svc.predict(X_test)\n",
        "y_pred_tree = tree_clf.predict(X_test)\n",
        "\n",
        "\n",
        "log_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\n",
        "kneighbors_cf = confusion_matrix(y_test, y_pred_knear)\n",
        "svc_cf = confusion_matrix(y_test, y_pred_svc)\n",
        "tree_cf = confusion_matrix(y_test, y_pred_tree)\n",
        "\n",
        "fig, ax = plt.subplots(2, 2,figsize=(22,12))\n",
        "\n",
        "\n",
        "sns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.copper)\n",
        "ax[0, 0].set_title(\"Logistic Regression \\n Confusion Matrix\", fontsize=14)\n",
        "ax[0, 0].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
        "ax[0, 0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
        "\n",
        "sns.heatmap(kneighbors_cf, ax=ax[0][1], annot=True, cmap=plt.cm.copper)\n",
        "ax[0][1].set_title(\"KNearsNeighbors \\n Confusion Matrix\", fontsize=14)\n",
        "ax[0][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
        "ax[0][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
        "\n",
        "sns.heatmap(svc_cf, ax=ax[1][0], annot=True, cmap=plt.cm.copper)\n",
        "ax[1][0].set_title(\"Suppor Vector Classifier \\n Confusion Matrix\", fontsize=14)\n",
        "ax[1][0].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
        "ax[1][0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
        "\n",
        "sns.heatmap(tree_cf, ax=ax[1][1], annot=True, cmap=plt.cm.copper)\n",
        "ax[1][1].set_title(\"DecisionTree Classifier \\n Confusion Matrix\", fontsize=14)\n",
        "ax[1][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
        "ax[1][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bJzTvgBZb3sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "print('Logistic Regression:')\n",
        "print(classification_report(y_test, y_pred_log_reg))\n",
        "\n",
        "print('KNears Neighbors:')\n",
        "print(classification_report(y_test, y_pred_knear))\n",
        "\n",
        "print('Support Vector Classifier:')\n",
        "print(classification_report(y_test, y_pred_svc))\n",
        "\n",
        "print('Support Vector Classifier:')\n",
        "print(classification_report(y_test, y_pred_tree))"
      ],
      "metadata": {
        "id": "D_r8cX96b7Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Score in the test set of logistic regression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Logistic Regression with Under-Sampling\n",
        "y_pred = log_reg.predict(X_test)\n",
        "undersample_score = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "# Logistic Regression with SMOTE Technique (Better accuracy with SMOTE t)\n",
        "y_pred_sm = best_est.predict(original_Xtest)\n",
        "oversample_score = accuracy_score(original_ytest, y_pred_sm)\n",
        "\n",
        "\n",
        "d = {'Technique': ['Random UnderSampling', 'Oversampling (SMOTE)'], 'Score': [undersample_score, oversample_score]}\n",
        "final_df = pd.DataFrame(data=d)\n",
        "\n",
        "# Move column\n",
        "score = final_df['Score']\n",
        "final_df.drop('Score', axis=1, inplace=True)\n",
        "final_df.insert(1, 'Score', score)\n",
        "\n",
        "# Note how high is accuracy score it can be misleading!\n",
        "final_df"
      ],
      "metadata": {
        "id": "MvT_3EQ1b9Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eexO9Sq1b--B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}